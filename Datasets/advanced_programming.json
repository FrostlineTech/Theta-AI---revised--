[
  {
    "question": "What are design principles in software engineering?",
    "answer": "Design principles in software engineering are fundamental guidelines that help developers create maintainable, flexible, and robust code. The most prominent include: 1) SOLID principles - Single Responsibility (a class should have one reason to change), Open/Closed (open for extension but closed for modification), Liskov Substitution (subtypes must be substitutable for base types), Interface Segregation (clients shouldn't depend on interfaces they don't use), and Dependency Inversion (depend on abstractions, not concretions); 2) DRY (Don't Repeat Yourself) - avoid code duplication; 3) KISS (Keep It Simple, Stupid) - avoid unnecessary complexity; 4) YAGNI (You Aren't Gonna Need It) - implement things only when needed; 5) Separation of Concerns - divide programs into distinct sections addressing separate concerns; 6) Law of Demeter - objects should only communicate with immediate friends; and 7) Composition over Inheritance - favor object composition over class inheritance. Following these principles results in code that's easier to maintain, test, extend, and understand while reducing bugs and technical debt."
  },
  {
    "question": "How does parallel programming work?",
    "answer": "Parallel programming is the technique of executing multiple computations simultaneously to improve performance and efficiency. It works through several key mechanisms: 1) Task Parallelism - different threads or processes execute different tasks concurrently; 2) Data Parallelism - the same operation is performed on different pieces of data simultaneously; 3) Multithreading - multiple threads within a process execute concurrently, sharing the same memory space; 4) Multiprocessing - multiple processes execute independently with separate memory spaces. Parallel programs face challenges including race conditions (when results depend on execution timing), deadlocks (processes waiting for each other indefinitely), thread synchronization, and load balancing. Common parallel programming models include threading libraries (pthreads, Java threads), OpenMP for shared memory parallelism, MPI for distributed memory systems, CUDA and OpenCL for GPU computing, and actor models for concurrent message passing. Effective parallel programming requires careful consideration of task division, communication overhead, synchronization points, and hardware architecture to achieve optimal speedup."
  },
  {
    "question": "What are immutable data structures?",
    "answer": "Immutable data structures are objects whose state cannot be modified after creation. Any operation that appears to modify an immutable structure actually creates and returns a new instance with the updated values while leaving the original unchanged. Key characteristics include thread safety (safe for concurrent access without locks), referential transparency (operations always produce the same output for the same input), and predictability (no unexpected side effects). Common implementations include strings in many languages, persistent data structures in functional languages, and specialized libraries for mainstream languages. While immutable structures offer benefits in concurrent programming, functional programming, and simplified debugging, they can have drawbacks including potential memory overhead from creating new instances and performance considerations for large data sets. Modern implementations like structural sharing (where new versions share unmodified portions with previous versions) help mitigate these concerns. Languages like Haskell, Clojure, and Scala have built-in support for immutability, while others like Java, JavaScript, and Python offer immutable collections through libraries or patterns."
  },
  {
    "question": "What is asynchronous programming?",
    "answer": "Asynchronous programming is a programming paradigm that allows operations to execute independently of the main program flow, enabling non-blocking execution that improves responsiveness and throughput, especially for I/O-bound operations. Key concepts include: 1) Callbacks - functions passed as arguments to be executed upon operation completion; 2) Promises/Futures - objects representing eventual completion (or failure) of operations; 3) Async/Await - syntactic sugar making asynchronous code resemble synchronous code while maintaining non-blocking behavior; 4) Event loops - mechanisms that handle the execution of multiple chunks of a program over time. Different languages implement asynchronous programming differently: JavaScript uses callbacks, promises, and async/await with a single-threaded event loop; Python offers asyncio and async/await syntax; C# provides Task objects and async/await; and Rust uses Future traits and async/await. Common challenges include callback hell (deeply nested callbacks), error handling complexity, debugging difficulties, and race conditions. Best practices include avoiding deep nesting, proper error handling, limiting shared state, and understanding the underlying concurrency model of the language or framework being used."
  },
  {
    "question": "What is functional programming?",
    "answer": "Functional programming (FP) is a declarative programming paradigm that treats computation as the evaluation of mathematical functions while avoiding changing state and mutable data. Core principles include: 1) Pure functions - functions without side effects that always return the same output for the same input; 2) Immutability - data cannot be changed after creation; 3) First-class and higher-order functions - functions can be assigned to variables, passed as arguments, and returned from other functions; 4) Recursion - preferred over iterative loops for repetitive operations; 5) Function composition - building complex functions by combining simpler ones; and 6) Referential transparency - expressions can be replaced with their values without changing program behavior. Popular functional languages include Haskell, Clojure, Scala, F#, and Erlang, while mainstream languages like JavaScript, Python, and Java have incorporated functional features. Benefits of FP include easier reasoning about code, better testability, simpler concurrent programming due to immutability, and improved modularity. However, challenges can include the learning curve, potential performance overhead, and integration with existing imperative or object-oriented codebases."
  },
  {
    "question": "What is WebAssembly?",
    "answer": "WebAssembly (Wasm) is a binary instruction format and low-level virtual machine that enables high-performance code execution in web browsers. It serves as a portable compilation target for languages like C, C++, Rust, and others, allowing developers to run near-native speed applications in browsers without plugins. Key features include: 1) Performance comparable to native code through binary format and efficient execution; 2) Security through a sandboxed execution environment with the same origin and permissions policies as JavaScript; 3) Language-agnostic design supporting multiple programming languages; 4) Compatibility with the web platform, working alongside JavaScript. WebAssembly modules are downloaded, compiled, and executed by browsers, with access to the same Web APIs available to JavaScript. Use cases include computationally intensive applications (gaming, video editing, CAD), porting existing C/C++/Rust applications to the web, and performance-critical parts of web applications. Beyond browsers, WebAssembly is expanding to server-side applications, edge computing, IoT devices, and blockchain platforms through initiatives like WASI (WebAssembly System Interface) that provide standardized system interfaces for non-browser environments."
  },
  {
    "question": "What is memory management in programming?",
    "answer": "Memory management is the process of controlling and coordinating computer memory allocation to programs, optimizing overall system performance. There are two primary approaches: manual memory management, where programmers explicitly allocate and deallocate memory (like in C with malloc/free), and automatic memory management, where the runtime environment handles memory (like garbage collection in Java or reference counting in Python). Key concepts include: stack memory (fast, limited, automatic) for local variables; heap memory (larger, slower, dynamic) for objects with variable lifetimes; memory leaks when allocated memory isn't freed; fragmentation when free memory becomes divided into small, non-contiguous blocks; and cache optimization for performance. Common memory management issues include dangling pointers (referencing freed memory), double freeing (attempting to free already deallocated memory), and buffer overflows (writing beyond allocated memory). Best practices include using appropriate data structures, releasing resources when done, employing smart pointers in C++, understanding language-specific memory models, and utilizing memory profiling tools to identify leaks and inefficiencies."
  },
  {
    "question": "What are design patterns in software development?",
    "answer": "Design patterns are standardized, reusable solutions to common software design problems that emerge in specific contexts. They represent best practices evolved through collective experience of software developers. Patterns are categorized into three main groups: 1) Creational patterns handle object creation mechanisms (examples: Singleton - ensures a class has only one instance; Factory Method - creates objects without specifying exact class; Builder - constructs complex objects step by step; Prototype - creates objects by copying existing ones; Abstract Factory - creates families of related objects); 2) Structural patterns focus on object composition and relationships (examples: Adapter - allows incompatible interfaces to work together; Decorator - adds responsibilities dynamically; Facade - provides simplified interface to subsystems; Composite - treats groups of objects as single objects; Proxy - controls access to objects); 3) Behavioral patterns manage algorithms and responsibility assignments (examples: Observer - notifies dependents about state changes; Strategy - encapsulates interchangeable algorithms; Command - turns requests into stand-alone objects; Iterator - provides sequential access without exposing underlying structure; State - alters object behavior when state changes). Benefits include established solutions to recurring problems, common vocabulary among developers, and improved code maintainability, while potential drawbacks include unnecessary complexity and performance overhead if misapplied."
  },
  {
    "question": "What is the CAP theorem?",
    "answer": "The CAP theorem (also known as Brewer's theorem) states that a distributed data store cannot simultaneously provide more than two out of these three guarantees: Consistency (every read receives the most recent write or an error), Availability (every request receives a non-error response, without guaranteeing it contains the most recent information), and Partition tolerance (the system continues operating despite network partitions or communication breakdowns between nodes). Since network partitions are unavoidable in distributed systems, designers must choose between CP (consistency and partition tolerance) or AP (availability and partition tolerance) systems. CP systems like traditional relational databases (PostgreSQL, MySQL) and consensus systems (etcd, Zookeeper) prioritize consistency, potentially becoming unavailable during partitions. AP systems like NoSQL databases (Cassandra, CouchDB) prioritize availability, potentially serving stale data during partitions. Modern distributed systems often implement nuanced approaches like tunable consistency levels, eventual consistency with conflict resolution, or hybrid approaches depending on specific requirements. When designing distributed systems, engineers must understand these trade-offs and choose appropriate consistency models based on application needs rather than attempting to circumvent the fundamental limitations described by the CAP theorem."
  },
  {
    "question": "What are microservices architecture principles?",
    "answer": "Microservices architecture principles guide the design and implementation of systems composed of small, independently deployable services. Key principles include: 1) Single Responsibility - each service handles one specific business capability; 2) Independence - services can be developed, deployed, and scaled independently; 3) Decentralization - favoring decentralized governance, data management, and decision-making; 4) Domain-Driven Design alignment - boundaries based on business domains rather than technical layers; 5) Resilience - designing for failure tolerance so individual service failures don't cascade; 6) Observable - comprehensive monitoring, logging, and tracing across services; 7) Automation - embracing CI/CD, infrastructure as code, and automated testing; 8) Smart endpoints, dumb pipes - complexity in services, not in communication; 9) API First - well-defined interfaces as contracts between services; 10) Evolutionary Design - allowing continuous refactoring and adaptation. Implementing these principles requires supporting practices like containerization, orchestration (Kubernetes), service meshes, API gateways, distributed tracing, and event-driven communication. Challenges include increased operational complexity, distributed system problems, service coordination issues, and data consistency concerns. Organizations should adopt microservices incrementally, starting with modular monoliths and evolving toward microservices as complexity justifies the architectural overhead."
  }
]
