[
  {
    "question": "What is the OSI model?",
    "answer": "The OSI (Open Systems Interconnection) model is a conceptual framework that standardizes the functions of a telecommunication or computing system into seven abstraction layers. From bottom to top: 1) Physical layer handles raw bit transmission over physical medium. 2) Data Link layer provides node-to-node data transfer and error detection/correction (includes MAC and LLC sublayers). 3) Network layer manages routing and logical addressing (IP) for data packets across multiple networks. 4) Transport layer ensures complete data transfer with segmentation, flow control, and error recovery (TCP/UDP protocols). 5) Session layer establishes, maintains, and terminates connections between applications. 6) Presentation layer translates between application and network formats, handling encryption, compression, and data conversion. 7) Application layer provides network services directly to end-users (HTTP, SMTP, FTP, etc.). While modern networks don't strictly follow all OSI layers (often using the simpler TCP/IP model), it remains valuable for understanding network operations, troubleshooting communication problems, and designing network security controls at appropriate layers."
  },
  {
    "question": "How do public key cryptography and digital signatures work?",
    "answer": "Public key cryptography (asymmetric encryption) uses mathematically related key pairs - a public key that can be shared and a private key kept secret. Data encrypted with one key can only be decrypted with its paired key. For secure communication, the sender encrypts a message using the recipient's public key, ensuring only the recipient's private key can decrypt it. Digital signatures work in reverse: the signer hashes a document and encrypts that hash with their private key, creating the signature. Anyone can verify the signature by decrypting it with the signer's public key and comparing the resulting hash with a freshly calculated hash of the document. If they match, this confirms both the signer's identity (authentication) and that the document hasn't been altered (integrity). The security relies on mathematical problems that are computationally infeasible to solve, such as integer factorization (RSA) or discrete logarithms (ECC). These systems form the foundation for secure internet communications, including HTTPS, secure email, cryptocurrency transactions, and code signing."
  },
  {
    "question": "Explain how DNS resolution works in detail",
    "answer": "DNS resolution is the process of converting human-readable domain names into IP addresses through several steps: 1) When a user enters a URL (www.example.com), the operating system first checks its local DNS cache for recent lookups. 2) If not found locally, it queries the recursive DNS resolver typically provided by the ISP or a service like Google DNS (8.8.8.8). 3) If the resolver doesn't have the information cached, it begins a recursive process starting with the root DNS servers (13 sets distributed globally). 4) The root server responds with addresses of TLD (Top-Level Domain) servers responsible for .com, .org, etc. 5) The resolver then queries the appropriate TLD server, which returns the authoritative nameservers for the specific domain (example.com). 6) The resolver contacts these authoritative nameservers to obtain the IP address for www.example.com. 7) The resolver returns this IP address to the user's device and caches it for future requests. 8) The browser can now establish a connection with the web server at that IP address. The entire process typically completes in milliseconds due to extensive caching at multiple levels. DNS also supports advanced features like round-robin load balancing, DNSSEC for security, and various record types (A, AAAA, MX, CNAME, TXT, etc.) for different purposes. DNS security is critical as attacks like cache poisoning, tunneling, and DDoS against DNS infrastructure can disrupt internet accessibility."
  },
  {
    "question": "How do compilers and interpreters work?",
    "answer": "Compilers and interpreters are tools that transform human-readable source code into executable machine code, but they operate differently. A compiler processes the entire source code in several phases: 1) Lexical analysis breaks code into tokens (keywords, identifiers, etc.). 2) Parsing creates an abstract syntax tree (AST) representing the code's grammatical structure. 3) Semantic analysis checks for type errors and other logical issues. 4) Optimization improves code efficiency. 5) Code generation translates the optimized representation into machine code specific to the target architecture. The resulting executable file can run independently without the compiler present, offering better performance but requiring recompilation for different platforms. In contrast, interpreters execute code line-by-line: they parse, analyze, and execute each statement immediately without producing an independent executable. This provides platform independence and immediate feedback, but with slower execution. Many modern language implementations use hybrid approaches: Java compiles to bytecode which then runs on the JVM (a virtual machine), while just-in-time (JIT) compilation in JavaScript engines dynamically compiles frequently used code paths during execution for optimized performance."
  },
  {
    "question": "What are design patterns in software engineering?",
    "answer": "Design patterns in software engineering are reusable solutions to common problems that occur during software design. They represent best practices evolved over time by experienced developers and provide templates for solving issues in specific contexts. Design patterns are categorized into three main types: 1) Creational patterns deal with object creation mechanisms (examples: Singleton ensures a class has only one instance; Factory Method creates objects without specifying exact class; Builder constructs complex objects step by step). 2) Structural patterns focus on relationships between entities (examples: Adapter allows incompatible interfaces to work together; Decorator adds responsibilities to objects dynamically; Composite treats groups of objects as single entities). 3) Behavioral patterns manage algorithms and communication between objects (examples: Observer notifies dependents of state changes; Strategy defines a family of interchangeable algorithms; Command encapsulates requests as objects). Using design patterns provides several benefits: they speed development through proven paradigms, improve code readability by establishing common vocabulary, enhance maintainability by following established principles like loose coupling and high cohesion, and facilitate communication among developers. However, they should be applied judiciously, as overuse or forcing patterns into inappropriate contexts can lead to unnecessary complexity."
  },
  {
    "question": "How does HTTPS encryption work?",
    "answer": "HTTPS encryption secures web communication through a combination of TLS (Transport Layer Security) protocol and traditional HTTP. The process works as follows: 1) When a browser connects to an HTTPS website, it initiates a TLS handshake. 2) The server sends its TLS certificate containing its public key and identity information, signed by a trusted Certificate Authority (CA). 3) The browser verifies the certificate's validity using pre-installed CA root certificates. 4) The browser generates a random symmetric session key, encrypts it with the server's public key, and sends it to the server. 5) Only the server can decrypt this session key using its private key, establishing a secure communication channel. 6) All subsequent HTTP traffic is encrypted and decrypted using this shared session key via symmetric encryption algorithms like AES, which are more efficient than asymmetric encryption for ongoing communication. This system provides three critical security properties: authentication (verifying the server's identity through certificates), confidentiality (protecting data from eavesdroppers via encryption), and integrity (ensuring data hasn't been modified in transit using MAC algorithms). Modern HTTPS implementations use TLS 1.2 or 1.3, offering perfect forward secrecy through ephemeral key exchanges like DHE or ECDHE, meaning compromised long-term keys can't decrypt previously recorded traffic."
  },
  {
    "question": "How does virtualization technology work?",
    "answer": "Virtualization technology creates isolated virtual environments that share underlying physical hardware resources. At its core is the hypervisor (virtual machine monitor), which comes in two types: Type 1 (bare-metal) runs directly on hardware (VMware ESXi, Microsoft Hyper-V), while Type 2 runs as an application on a host OS (VirtualBox, VMware Workstation). The hypervisor implements several key mechanisms: 1) CPU virtualization uses hardware extensions (Intel VT-x, AMD-V) to efficiently execute guest instructions and trap privileged operations. 2) Memory virtualization creates separate address spaces for each VM using techniques like shadow page tables or hardware-assisted paging (Intel EPT, AMD RVI) to map virtual memory to physical memory. 3) I/O virtualization handles device access through emulation (simulating hardware), paravirtualization (modified drivers aware of virtualization), or direct device assignment (passthrough). 4) Resource scheduling allocates CPU, memory, network, and storage resources among competing VMs. Modern virtualization extends beyond server virtualization to containers (Docker, Kubernetes) which share the OS kernel for lighter-weight isolation, network virtualization (SDN), and storage virtualization. Benefits include improved hardware utilization, isolation for security and stability, simplified disaster recovery, and the ability to run multiple operating systems concurrently. Virtual machines can be snapshotted, migrated between physical hosts, and easily replicated, making them fundamental building blocks of modern cloud infrastructure."
  },
  {
    "question": "What are the key principles of DevOps?",
    "answer": "DevOps represents a cultural and technical approach to software delivery based on several key principles: 1) Collaboration - breaking down silos between development and operations teams with shared responsibilities and goals. 2) Automation - implementing continuous integration, delivery, and deployment pipelines to reduce manual work and human error. 3) Continuous Integration - frequently merging code changes into a central repository followed by automated builds and tests. 4) Continuous Delivery - ensuring code is always in a deployable state through automated testing and validation. 5) Infrastructure as Code - managing infrastructure through version-controlled configuration files rather than manual processes. 6) Monitoring and Observability - implementing comprehensive telemetry across applications and infrastructure for proactive issue detection. 7) Feedback Loops - establishing mechanisms for quick feedback from operations to development and vice versa. 8) Microservices Architecture - decomposing applications into smaller, independently deployable services when appropriate. 9) Security Integration (DevSecOps) - incorporating security practices throughout the development lifecycle rather than as a final gate. 10) Continuous Improvement - fostering a culture of experimentation, learning from failures, and iterative enhancement. The technical implementation typically involves toolchains spanning source control (Git), CI/CD tools (Jenkins, GitHub Actions), configuration management (Ansible, Terraform), containerization (Docker, Kubernetes), and monitoring solutions (Prometheus, ELK stack). Successful DevOps adoption typically requires organizational change management alongside technical implementations to overcome traditional siloed structures and mindsets."
  },
  {
    "question": "How does garbage collection work in programming languages?",
    "answer": "Garbage collection (GC) is an automatic memory management technique that identifies and frees memory no longer referenced by a program. The process typically involves: 1) Marking - the collector identifies all reachable (live) objects by following references from root objects (global variables, stack variables, registers). 2) Sweeping - unreachable objects are reclaimed. 3) Compaction (optional) - live objects may be relocated to reduce fragmentation. Common GC algorithms include: Reference Counting tracks how many references point to each object and deallocates when the count reaches zero, providing immediate collection but struggling with circular references. Mark-and-Sweep performs the marking and sweeping phases described above, handling circular references but potentially causing program pauses. Generational collection optimizes performance by dividing objects into generations based on age, with younger generations collected more frequently since newly created objects tend to become garbage sooner (weak generational hypothesis). Concurrent and incremental collectors reduce pause times by performing collection work alongside the application or in small chunks. Languages implement GC differently: Java uses generational collection with various collector options, Python uses reference counting with cycle detection, JavaScript engines use mark-and-sweep variants, and Go employs a concurrent collector. While GC eliminates manual memory management bugs, it introduces overhead and non-deterministic pauses, making it unsuitable for certain real-time applications where languages like C/C++ with manual memory management are preferred."
  },
  {
    "question": "How do databases ensure ACID properties?",
    "answer": "Databases ensure ACID properties (Atomicity, Consistency, Isolation, Durability) through various mechanisms: Atomicity guarantees that transactions are all-or-nothing, implemented via write-ahead logging (WAL) where changes are recorded in a log before being applied to the database, enabling rollback if a transaction fails. Consistency maintains database integrity through constraints (primary keys, foreign keys, unique constraints) and triggers that validate changes before committing. Isolation prevents interference between concurrent transactions using locking mechanisms or multi-version concurrency control (MVCC). Locking can be pessimistic (acquiring locks before operations) or optimistic (detecting conflicts at commit time), with isolation levels ranging from Read Uncommitted (lowest isolation) to Serializable (highest isolation) controlling phenomena like dirty reads, non-repeatable reads, and phantom reads. Durability ensures committed transactions survive system failures by persisting transaction logs to non-volatile storage before acknowledging commits, often using techniques like synchronous disk writes and replication. These mechanisms involve trade-offs: stronger ACID guarantees typically reduce concurrency and performance. Different database systems implement these properties differently: traditional relational databases (PostgreSQL, Oracle) generally prioritize strong ACID compliance, while distributed databases may relax certain properties to gain scalability and availability, as described by the CAP theorem. Modern distributed databases often implement variations like BASE (Basically Available, Soft state, Eventually consistent) that provide weaker guarantees but better performance at scale."
  },
  {
    "question": "Explain how TCP/IP works",
    "answer": "TCP/IP (Transmission Control Protocol/Internet Protocol) is the foundational communication protocol suite for the internet, operating as a four-layer model: 1) The Link layer (equivalent to OSI Physical and Data Link layers) handles physical transmission and addressing using protocols like Ethernet and ARP. Each network interface has a MAC address for local identification. 2) The Internet layer routes packets across networks using IP (Internet Protocol). IPv4 and IPv6 provide logical addressing, with each device assigned an IP address. This layer handles fragmentation, reassembly, and routing through intermediate networks. 3) The Transport layer provides end-to-end communication between applications using primarily TCP or UDP. TCP offers reliable, connection-oriented delivery with features like: three-way handshake (SYN, SYN-ACK, ACK) to establish connections, sequence numbers to order packets, acknowledgments and retransmission for reliability, flow control (sliding window) to prevent overwhelming receivers, and congestion control algorithms (slow start, congestion avoidance) to manage network traffic. UDP provides connectionless, unreliable but faster delivery suitable for time-sensitive applications like video streaming. 4) The Application layer contains protocols that applications use directly, such as HTTP, SMTP, FTP, DNS, and SSH. Each connection is identified by a socket, combining IP address and port number. The entire system operates on packet switching, where data is divided into packets that may take different routes to their destination. This distributed approach provides resilience, as traffic can route around network failures. The protocol suite's design principles of stateless routers, end-to-end reliability, and layered abstraction have enabled the internet's remarkable scalability."
  },
  {
    "question": "How do neural networks learn?",
    "answer": "Neural networks learn through a process called backpropagation coupled with optimization algorithms like gradient descent. The process works as follows: 1) Initialization - weights and biases are set randomly. 2) Forward propagation - input data passes through the network, with each neuron computing a weighted sum of inputs plus bias, then applying an activation function (like ReLU, sigmoid, or tanh). 3) Loss calculation - the network's output is compared to the desired output using a loss function (e.g., mean squared error for regression, cross-entropy for classification). 4) Backpropagation - the gradient of the loss with respect to each weight is calculated using the chain rule of calculus, efficiently computing how much each parameter contributed to the error. 5) Parameter update - weights and biases are adjusted in the opposite direction of the gradient to reduce the loss, using optimization algorithms that determine the step size (learning rate). This process repeats across many examples (mini-batches) and multiple passes through the entire dataset (epochs). Advanced techniques improve this basic approach: adaptive optimizers like Adam adjust learning rates per parameter; regularization methods like dropout and weight decay prevent overfitting; batch normalization stabilizes and accelerates training by normalizing layer inputs; residual connections enable training of very deep networks by providing gradient shortcuts. The learning process requires carefully balancing network capacity, dataset size, and regularization to achieve good generalization to unseen data. Modern neural networks may contain millions or billions of parameters and require specialized hardware like GPUs or TPUs to train efficiently."
  }
]
